---
title: "p8105_hw2_xh2601"
author: "Samantha Xinwei Han"
date: "2025-09-29"
output: github_document
---

```{r setup, message = FALSE}
# load the tidyverse and readxl library
library(tidyverse)
library(readxl)
```

## Problem 1

#### Step One. Clean the data in pols-month.csv
1. Use separate() to break up the variable mon into integer variables year, month, and day

```{r}
# create a dataframe named "pols" for pols-month.csv data
# use 'clean_names()' to clean variable names
# use 'separate()' to break "mon" variable into "year", "month", "day" variables, separated by "-"
# use 'mutate()' to convert character variables into integer variables
pols = 
  read_csv("data/fivethirtyeight_datasets/pols-month.csv") |> 
  janitor::clean_names() |> 
  separate(mon, into = c("year", "month", "day"), sep = "-") |> 
  mutate(
    year = as.integer(year),
    month = as.integer(month),
    day = as.integer(day)
  )
```

2. Replace month number with month name

```{r}
# use 'mutate()' to replace month number with month name in "month"
pols = mutate(
  pols,
  month = case_match(
    month,
    1 ~ "January",
    2 ~ "February",
    3 ~ "March",
    4 ~ "April",
    5 ~ "May",
    6 ~ "June",
    7 ~ "July",
    8 ~ "August",
    9 ~ "September",
    10 ~ "October",
    11 ~ "November",
    12 ~ "December"
  )
)
```

3. Create a president variable taking values gop and dem, and remove prez_dem and prez_gop

```{r}
# use 'mutate()' to create "president" variable, so that if "prez_gop" is 1, "president" is "gop", and if "prez_dem" is 1, "president" is "dem"
# use 'select()' to remove "prez_dem" and "prez_gop"
# use 'select()' to reorder the dataframe, so that "president" is in the front
pols = pols |> 
  mutate(
    president = case_when(
      prez_gop == 1 ~ "gop",
      prez_dem == 1 ~ "dem"
    )
  ) |> 
  select(-prez_dem, -prez_gop) |> 
  select(year, month, day, president, everything())
```

4. Remove the day variable

```{r}
# use 'select()' to remove "day"
pols = select(pols, -day)
```

#### Step Two. Clean the data in snp.csv

```{r}
# create a dataframe named "snp" for snp.csv data
# use 'clean_names()' to clean variable names
# use 'separate()' to break "date" variable into "year", "month", "day" variables, separated by "/"
# use 'mutate()' to convert character variables into integer variables
# use 'mutate()' to replace month number with month name in "month"
# use 'mutate()' to replace year number with the full year
# use 'select()' to remove "day"
# use 'select()' to reorder the dataframe, so that "year" and "month" are the leading cols
snp = 
  read_csv("data/fivethirtyeight_datasets/snp.csv") |> 
  janitor::clean_names() |> 
  separate(date, into = c("month", "day", "year"), sep = "/") |> 
  mutate(
    year = as.integer(year),
    month = as.integer(month),
    day = as.integer(day)
  ) |> 
  mutate(
    month = case_match(
      month,
      1 ~ "January",
      2 ~ "February",
      3 ~ "March",
      4 ~ "April",
      5 ~ "May",
      6 ~ "June",
      7 ~ "July",
      8 ~ "August",
      9 ~ "September",
      10 ~ "October",
      11 ~ "November",
      12 ~ "December"
    )
  ) |> 
  mutate(
    year = if_else(year < 25, year + 2000, year + 1900)
  ) |> 
  select(-day) |> 
  select(year, month, everything())
```

#### Step Three. Tidy the unemployment data

```{r}
# create a dataframe named "unemployment" for unemployment.csv data
# use 'clean_names()' to clean variable names
# use 'pivot_longer()' to convert wide to long format
# use 'mutate()' to convert character variable into integer variable in "year"
# use 'mutate()' to replace abbreviated month with month name in "month"
unemployment = 
  read_csv("data/fivethirtyeight_datasets/unemployment.csv") |> 
  janitor::clean_names() |> 
  pivot_longer(
    cols = jan:dec,
    names_to = "month",
    values_to = "unemployment_percentage"
  ) |> 
  mutate(
    year = as.integer(year)
  ) |> 
  mutate(
    month = case_match(
      month,
      "jan" ~ "January",
      "feb" ~ "February",
      "mar" ~ "March",
      "apr" ~ "April",
      "may" ~ "May",
      "jun" ~ "June",
      "jul" ~ "July",
      "aug" ~ "August",
      "sep" ~ "September",
      "oct" ~ "October",
      "nov" ~ "November",
      "dec" ~ "December"
    )
  )
```

#### Step Four. Join the datasets and interpret

```{r}
# use 'left_join()' to merge snp into pols
# use 'left_join()' to merge unemployment into the previous result
pol_econ_df = 
  left_join(pols, snp, by = c("year", "month")) |> 
  left_join(unemployment, by = c("year", "month"))
```

**Interpretation:**

*pols*: contains 822 observations of 9 variables related to the number of politicians who are democratic or republican at a given time. 

- "year" and "month" record the year and month of the count; 
- "president" indicate whether the president was republican or democratic on the associated year and month (gop = republican, dem = democratic); 
- "gov_gop", "sen_gop", and "rep_gop" record the number of republican governors, senators, and representatives, respectively, on the associated year and month;
- "gov_dem", "sen_dem", and "rep_dem" record the number of democratic governors, senators, and representatives, respectively, on the associated year and month.

*snp*: contains 787 observations of 3 variables related to Standard & Poorâ€™s stock market index (S&P), often used as a representative measure of stock market as a whole, at a given time.

- "year" and "month" record the year and month of the observation;
- "close" records the closing values of the S&P stock index on the associated year and month.

*unemployment*: contains 816 observations of 3 variables related to the percentage of unemployment at a given time.

- "year" and "month" record the year and month of the measurement;
- "unemployment_percentage" records the percentage of unemployment on the associated year and month.

*pol_econ_df*: contains 822 observations of 11 variables combining variables from *pols*, *snp*, and *unemployment* datasets.

- dimension: 822 rows, 11 columns.
- range of years: from January 1947 to June 2015. This range is determined by the *pols* dataset, while variables from *snp* and *unemployment* are included only for months and years where their data overlap with this time period.
- names of key variables: 
  - "year" and "month" record the year and month of the observation, which are form the common time index across all three datasets.
  - Each observation could include the party of the president, the number of Democratic and Republican politicians, S&P index, and unemployment rate, provided that valid data are available from the three source datasets for that year and month.
- *pol_econ_df* is a monthly dataset spanning January 1947 to June 2015, containing political and economic related data. It allows us to learn the relations between political power (specifically, the party holding presidency and party composition of politicians) and economic indicators including stock market performance (S&P index) and the unemployment rate.

## Problem 2

#### Step One. Clean the Mr. Trash Wheel sheet

```{r}
# use 'read_excel()' to read the file
# use 'sheet' to specify the sheet 
# use 'skip' to omit non-data entries - top row
# use 'range' to omit non-data entries - columns after the 14th column
# use 'clean_names()' to clean variable names
# use 'drop_na()' to omit rows that do not include dumpster-specific data
# use 'mutate()' to round the number of sports balls to the nearest integer
# use 'mutate()' to convert the previous result to an integer
# use 'mutate()' to convert year to an integer
# use 'mutate()' to create a variable to keep track of which Trash Wheel is which 
mr_tw_df = 
  read_excel(
    "data/202509 Trash Wheel Collection Data.xlsx", 
    sheet = "Mr. Trash Wheel",
    skip = 1,
    range = cell_cols(1:14)) |> 
  janitor::clean_names() |> 
  drop_na(dumpster) |> 
  mutate(sports_balls = round(sports_balls)) |> 
  mutate(sports_balls = as.integer(sports_balls)) |> 
  mutate(year = as.integer(year)) |> 
  mutate(trash_wheel_type = "mr_trash_wheel")
```

#### Step Two. Clean the Professor Trash Wheel sheet

```{r}
prof_tw_df = 
  read_excel(
    "data/202509 Trash Wheel Collection Data.xlsx", 
    sheet = "Professor Trash Wheel",
    skip = 1,
    range = cell_cols(1:13)) |> 
  janitor::clean_names() |> 
  drop_na(dumpster) |> 
  mutate(year = as.integer(year)) |>
  mutate(trash_wheel_type = "professor_trash_wheel")
```

#### Step Three. Clean the Gwynnda Trash Wheel sheet

```{r}
gwyn_tw_df = 
  read_excel(
    "data/202509 Trash Wheel Collection Data.xlsx", 
    sheet = "Gwynns Falls Trash Wheel",
    skip = 1,
    range = cell_cols(1:12)) |> 
  janitor::clean_names() |> 
  drop_na(dumpster) |> 
  mutate(year = as.integer(year)) |>
  mutate(trash_wheel_type = "gwynnda_trash_wheel") 
```

#### Step Four. Combine the three Trash Wheel sheets

```{r}
# use 'bind_rows()' to combine three datasets
# use 'select()' to reorder the dataframe, so that "trash_wheel_type" is in the front
combined_tw_df = 
  bind_rows(mr_tw_df, prof_tw_df, gwyn_tw_df) |> 
  select(dumpster, trash_wheel_type, year, everything())
```

#### Step Five. Interpret data

```{r}
# total weight of trash collected by Professor Trash Wheel
wt_prof_tw = sum(mr_tw_df[["weight_tons"]], na.rm = TRUE)
  
# total number of cigarette butts collected by Gwynnda in June of 2022
cig_2206_gwyn_tw = gwyn_tw_df |> 
  filter(year == 2022, month == "June") |> 
  pull(cigarette_butts) |> 
  sum(na.rm = TRUE)
```

**Interpretation:**

- There are 1188 observations and 15 columns in the combined dataset (*combined_tw_df*), including 707 observations from the Mr. Trash Wheel dataset (*mr_tw_df*), 132 observations from the Professor Trash Wheel dataset (*prof_tw_df*), and 349 observations from the Gwynns Falls Trash Wheel dataset (*gwyn_tw_df*).

- *mr_tw_df* contains 15 columns: dumpster ID; year, month, and date of observation; total weight (in tons) and volume (in cubic yards) of trash on the observation date and dumpster id; numbers of plastic bottles, polystyrene, cigarette butts, glass bottles, plastic bags, wrappers, and sports balls on the observation date and dumpster id; estimated number of homes that could have been powered by the collected trash; and the type/name of trash wheel.

- *prof_tw_df* contains 14 columns similar to *mr_tw_df*, but without the number of sports balls. 

- *gwyn_tw_df* contains 13 columns similar to *mr_tw_df*, but without the number of sports balls or glass bottles. 

- The total weight of trash collected by Professor Trash Wheel is `r wt_prof_tw` tons.

- The total number of cigarette butts collected by Gwynnda in June of 2022 is `r format(cig_2206_gwyn_tw, scientific = FALSE)`.

## Problem 3

#### Step One. Clean zip codes data

```{r}
# create a dataframe named "zip_code_df" for Zip Codes.csv data
# use 'clean_names()' to clean variable names
# use 'mutate()' to make sure FIPS and zip code variables are integer variables
# use 'select()' to reorder the dataframe, so that the date and state information are leading, then county info, then neighborhood info
# use 'rename()' to rename file_date and county for clarity
# use 'mutate()' to rename the date format for consistency with the zori_df
zip_code_df = 
  read_csv("data/zillow_data/Zip Codes.csv") |> 
  janitor::clean_names() |> 
  mutate(
    state_fips = as.integer(state_fips),
    county_fips = as.integer(county_fips),
    zip_code = as.integer(zip_code)
  ) |> 
  select(file_date, state_fips, county, county_code, county_fips, zip_code, neighborhood) |> 
  rename(zip_code_date = file_date, county_name_zipcode = county) |> 
  mutate(zip_code_date = "2007_07_25")

# check for uniqueness of zip code
duplicated_zip_code = zip_code_df |> 
  group_by(zip_code) |> 
  filter(n() > 1)
```

By reviewing the duplicated_zip_code table, we identify two repeated zip_code data (10463 and 11201). After checking, 10463 should be in Bronx, not New York County; 11201 should be in Kings, not New York County. Therefore, we remove the corresponded rows with the incorrect data.

```{r}
# use 'filter()' to remove the two incorrect rows
zip_code_df = zip_code_df |> 
  filter(!(zip_code == "11201" & county_name_zipcode == "New York"),
         !(zip_code == "10463" & county_name_zipcode == "New York"))
```

#### Step Two. Clean ZORI data

```{r}
# create a dataframe named "zori_df" for Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv data
# use 'clean_names()' to clean variable names
# use 'mutate()' to make sure some variables are integer variables
# use 'rename()' to rename region_name to be consistent with zip_code_df, and county_name, city for clarity
# use 'select()' to remove region_type (included when we rename region_name to zip_code) and state (repeated variable as state_name)
# use 'mutate()' to remove " County" in county_name to be consistent with zip_code_df
# use 'rename_with()' to rename the date variables for clarity
zori_df = 
  read_csv("data/zillow_data/Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv") |> 
  janitor::clean_names() |> 
  mutate(
    region_id = as.integer(region_id),
    size_rank = as.integer(size_rank),
    region_name = as.integer(region_name)
  ) |>
  rename(zip_code = region_name, county_name_zori = county_name, city_name = city) |> 
  select(-region_type, -state) |> 
  mutate(county_name_zori = str_remove(county_name_zori, " County$")) |> 
  rename_with(~ sub("^x", "", .x))

# check for uniqueness of zip code
duplicated_zori = zori_df |> 
  group_by(zip_code) |> 
  filter(n() > 1)
# there's no duplicated zip code!

# use pivot_longer() to convert wide to long data format for tidiness and readability 
# use 'separate()' to break "date" variable into "year", "month", "day" variables, separated by "_"
# use 'mutate()' to convert date into integer variables
# use 'mutate()' to replace month number with month name in "month"
# use 'select()' to remove the "day" variable, because it only represents the last day of the month
# use 'select()' to reorder the dataframe
zori_df = zori_df |> 
  pivot_longer(
    cols = "2015_01_31":"2024_08_31",
    names_to = "date",
    values_to = "zori"
  ) |> 
  separate(date, into = c("year", "month", "day"), sep = "_") |> 
  mutate(
    year = as.integer(year),
    month = as.integer(month),
    day = as.integer(day)
  ) |> 
  mutate(
    month = case_match(
    month,
    1 ~ "January",
    2 ~ "February",
    3 ~ "March",
    4 ~ "April",
    5 ~ "May",
    6 ~ "June",
    7 ~ "July",
    8 ~ "August",
    9 ~ "September",
    10 ~ "October",
    11 ~ "November",
    12 ~ "December"
    )
  ) |> 
  select(-day) |> 
  select(year, month, metro, state_name, city_name, county_name_zori, zip_code, region_id, size_rank, zori)
```

#### Step Three. Combine and interpret

```{r}
# create a dataframe named "zori_w_zip_df" for the combined data
# use 'left_join()' to merge zori data into the zip code data
# use 'select()' to reorder the dataframe, so that it has the order of: date -> metro area -> state info -> city info -> county info -> zip code & region info -> zori by date
# use 'mutate()' to fill in the incomplete metro, state_name, and city_name data
zori_w_zip_df = 
  left_join(zip_code_df, zori_df, by = "zip_code") |> 
  select(zip_code_date, metro, state_name, state_fips, city_name, county_name_zipcode, county_name_zori, county_code, county_fips, region_id, zip_code, neighborhood, size_rank, everything()) |> 
  mutate(metro = "New York-Newark-Jersey City, NY-NJ-PA",
         state_name = "NY",
         city_name = "New York") 
  

### next, check for inconsistency between county_name_zipcode and county_name_zori
```



Briefly describe the resulting tidy dataset. 
How many total observations exist? 
How many unique ZIP codes are included, and how many unique neighborhoods?

Which ZIP codes appear in the ZIP code dataset but not in the Zillow Rental Price dataset? 
Using a few illustrative examples discuss why these ZIP codes might be excluded from the Zillow dataset.

For all available ZIP codes, compare rental prices in January 2021 to prices in January 2020. 
Make a table that shows the 10 ZIP codes (along with the borough and neighborhood) with largest drop in price from January 2020 to 2021. 






