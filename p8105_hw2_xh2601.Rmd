---
title: "p8105_hw2_xh2601"
author: "Samantha Xinwei Han"
date: "2025-09-29"
output: github_document
---

```{r setup, message = FALSE}
# load the tidyverse and readxl library
library(tidyverse)
library(readxl)
```

## Problem 1

#### Step One. Clean the data in pols-month.csv
1. Use separate() to break up the variable mon into integer variables year, month, and day

```{r}
# create a dataframe named "pols" for pols-month.csv data
# use 'clean_names()' to clean variable names
# use 'separate()' to break "mon" variable into "year", "month", "day" variables, separated by "-"
# use 'mutate()' to convert character variables into integer variables
pols = 
  read_csv("data/fivethirtyeight_datasets/pols-month.csv") |> 
  janitor::clean_names() |> 
  separate(mon, into = c("year", "month", "day"), sep = "-") |> 
  mutate(
    year = as.integer(year),
    month = as.integer(month),
    day = as.integer(day)
  )
```

2. Replace month number with month name

```{r}
# use 'mutate()' to replace month number with month name in "month"
pols = mutate(
  pols,
  month = case_match(
    month,
    1 ~ "January",
    2 ~ "February",
    3 ~ "March",
    4 ~ "April",
    5 ~ "May",
    6 ~ "June",
    7 ~ "July",
    8 ~ "August",
    9 ~ "September",
    10 ~ "October",
    11 ~ "November",
    12 ~ "December"
  )
)
```

3. Create a president variable taking values gop and dem, and remove prez_dem and prez_gop

```{r}
# use 'mutate()' to create "president" variable, so that if "prez_gop" is 1, "president" is "gop", and if "prez_dem" is 1, "president" is "dem"
# use 'select()' to remove "prez_dem" and "prez_gop"
# use 'select()' to reorder the dataframe, so that "president" is in the front
pols = pols |> 
  mutate(
    president = case_when(
      prez_gop == 1 ~ "gop",
      prez_dem == 1 ~ "dem"
    )
  ) |> 
  select(-prez_dem, -prez_gop) |> 
  select(year, month, day, president, everything())
```

4. Remove the day variable

```{r}
# use 'select()' to remove "day"
pols = select(pols, -day)
```

#### Step Two. Clean the data in snp.csv

```{r}
# create a dataframe named "snp" for snp.csv data
# use 'clean_names()' to clean variable names
# use 'separate()' to break "date" variable into "year", "month", "day" variables, separated by "/"
# use 'mutate()' to convert character variables into integer variables
# use 'mutate()' to replace month number with month name in "month"
# use 'mutate()' to replace year number with the full year
# use 'select()' to remove "day"
# use 'select()' to reorder the dataframe, so that "year" and "month" are the leading cols
snp = 
  read_csv("data/fivethirtyeight_datasets/snp.csv") |> 
  janitor::clean_names() |> 
  separate(date, into = c("month", "day", "year"), sep = "/") |> 
  mutate(
    year = as.integer(year),
    month = as.integer(month),
    day = as.integer(day)
  ) |> 
  mutate(
    month = case_match(
      month,
      1 ~ "January",
      2 ~ "February",
      3 ~ "March",
      4 ~ "April",
      5 ~ "May",
      6 ~ "June",
      7 ~ "July",
      8 ~ "August",
      9 ~ "September",
      10 ~ "October",
      11 ~ "November",
      12 ~ "December"
    )
  ) |> 
  mutate(
    year = if_else(year < 25, year + 2000, year + 1900)
  ) |> 
  select(-day) |> 
  select(year, month, everything())
```

#### Step Three. Tidy the unemployment data

```{r}
# create a dataframe named "unemployment" for unemployment.csv data
# use 'clean_names()' to clean variable names
# use 'pivot_longer()' to convert wide to long format
# use 'mutate()' to convert character variable into integer variable in "year"
# use 'mutate()' to replace abbreviated month with month name in "month"
unemployment = 
  read_csv("data/fivethirtyeight_datasets/unemployment.csv") |> 
  janitor::clean_names() |> 
  pivot_longer(
    cols = jan:dec,
    names_to = "month",
    values_to = "unemployment_percentage"
  ) |> 
  mutate(
    year = as.integer(year)
  ) |> 
  mutate(
    month = case_match(
      month,
      "jan" ~ "January",
      "feb" ~ "February",
      "mar" ~ "March",
      "apr" ~ "April",
      "may" ~ "May",
      "jun" ~ "June",
      "jul" ~ "July",
      "aug" ~ "August",
      "sep" ~ "September",
      "oct" ~ "October",
      "nov" ~ "November",
      "dec" ~ "December"
    )
  )
```

#### Step Four. Join the datasets and interpret

```{r}
# use 'left_join()' to merge snp into pols
# use 'left_join()' to merge unemployment into the previous result
pol_econ_df = 
  left_join(pols, snp, by = c("year", "month")) |> 
  left_join(unemployment, by = c("year", "month"))
```

**Interpretation:**

*pols*: contains 822 observations of 9 variables related to the number of politicians who are democratic or republican at a given time. 

- "year" and "month" record the year and month of the count; 
- "president" indicate whether the president was republican or democratic on the associated year and month (gop = republican, dem = democratic); 
- "gov_gop", "sen_gop", and "rep_gop" record the number of republican governors, senators, and representatives, respectively, on the associated year and month;
- "gov_dem", "sen_dem", and "rep_dem" record the number of democratic governors, senators, and representatives, respectively, on the associated year and month.

*snp*: contains 787 observations of 3 variables related to Standard & Poorâ€™s stock market index (S&P), often used as a representative measure of stock market as a whole, at a given time.

- "year" and "month" record the year and month of the observation;
- "close" records the closing values of the S&P stock index on the associated year and month.

*unemployment*: contains 816 observations of 3 variables related to the percentage of unemployment at a given time.

- "year" and "month" record the year and month of the measurement;
- "unemployment_percentage" records the percentage of unemployment on the associated year and month.

*pol_econ_df*: contains 822 observations of 11 variables combining variables from *pols*, *snp*, and *unemployment* datasets.

- dimension: 822 rows, 11 columns.
- range of years: from January 1947 to June 2015. This range is determined by the *pols* dataset, while variables from *snp* and *unemployment* are included only for months and years where their data overlap with this time period.
- names of key variables: 
  - "year" and "month" record the year and month of the observation, which are form the common time index across all three datasets.
  - Each observation could include the party of the president, the number of Democratic and Republican politicians, S&P index, and unemployment rate, provided that valid data are available from the three source datasets for that year and month.
- *pol_econ_df* is a monthly dataset spanning January 1947 to June 2015, containing political and economic related data. It allows us to learn the relations between political power (specifically, the party holding presidency and party composition of politicians) and economic indicators including stock market performance (S&P index) and the unemployment rate.

## Problem 2

#### Step One. Clean the Mr. Trash Wheel sheet

```{r}
# use 'read_excel()' to read the file
# use 'sheet' to specify the sheet 
# use 'skip' to omit non-data entries - top row
# use 'range' to omit non-data entries - columns after the 14th column
# use 'clean_names()' to clean variable names
# use 'drop_na()' to omit rows that do not include dumpster-specific data
# use 'mutate()' to round the number of sports balls to the nearest integer
# use 'mutate()' to convert the previous result to an integer
# use 'mutate()' to convert year to an integer
# use 'mutate()' to create a variable to keep track of which Trash Wheel is which 
mr_tw_df = 
  read_excel(
    "data/202509 Trash Wheel Collection Data.xlsx", 
    sheet = "Mr. Trash Wheel",
    skip = 1,
    range = cell_cols(1:14)) |> 
  janitor::clean_names() |> 
  drop_na(dumpster) |> 
  mutate(sports_balls = round(sports_balls)) |> 
  mutate(sports_balls = as.integer(sports_balls)) |> 
  mutate(year = as.integer(year)) |> 
  mutate(trash_wheel_type = "mr_trash_wheel")
```

#### Step Two. Clean the Professor Trash Wheel sheet

```{r}
prof_tw_df = 
  read_excel(
    "data/202509 Trash Wheel Collection Data.xlsx", 
    sheet = "Professor Trash Wheel",
    skip = 1,
    range = cell_cols(1:13)) |> 
  janitor::clean_names() |> 
  drop_na(dumpster) |> 
  mutate(year = as.integer(year)) |>
  mutate(trash_wheel_type = "professor_trash_wheel")
```

#### Step Three. Clean the Gwynns Falls Trash Wheel sheet

```{r}
gwyn_tw_df = 
  read_excel(
    "data/202509 Trash Wheel Collection Data.xlsx", 
    sheet = "Gwynns Falls Trash Wheel",
    skip = 1,
    range = cell_cols(1:12)) |> 
  janitor::clean_names() |> 
  drop_na(dumpster) |> 
  mutate(year = as.integer(year)) |>
  mutate(trash_wheel_type = "gwynnda_trash_wheel") 
```

#### Step Four. Combine the three Trash Wheel sheets

```{r}
# use 'bind_rows()' to combine three datasets
# use 'select()' to reorder the dataframe, so that "trash_wheel_type" is in the front
combined_tw_df = 
  bind_rows(mr_tw_df, prof_tw_df, gwyn_tw_df) |> 
  select(dumpster, trash_wheel_type, year, everything())
```

#### Step Five. Interpret data

```{r}
# total weight of trash collected by Professor Trash Wheel
wt_prof_tw = sum(prof_tw_df[["weight_tons"]], na.rm = TRUE)
  
# total number of cigarette butts collected by Gwynnda in June of 2022
cig_2206_gwyn_tw = gwyn_tw_df |> 
  filter(year == 2022, month == "June") |> 
  pull(cigarette_butts) |> 
  sum(na.rm = TRUE)
```

**Interpretation:**

- There are 1188 observations and 15 columns in the combined dataset (*combined_tw_df*), including 707 observations from the Mr. Trash Wheel dataset (*mr_tw_df*), 132 observations from the Professor Trash Wheel dataset (*prof_tw_df*), and 349 observations from the Gwynns Falls Trash Wheel dataset (*gwyn_tw_df*).

- *mr_tw_df* contains 15 columns: dumpster ID; year, month, and date of observation; total weight (in tons) and volume (in cubic yards) of trash on the observation date and dumpster id; numbers of plastic bottles, polystyrene, cigarette butts, glass bottles, plastic bags, wrappers, and sports balls on the observation date and dumpster id; estimated number of homes that could have been powered by the collected trash; and the type/name of trash wheel.

- *prof_tw_df* contains 14 columns similar to *mr_tw_df*, but without the number of sports balls. 

- *gwyn_tw_df* contains 13 columns similar to *mr_tw_df*, but without the number of sports balls or glass bottles. 

- The total weight of trash collected by Professor Trash Wheel is `r wt_prof_tw` tons.

- The total number of cigarette butts collected by Gwynnda in June of 2022 is `r format(cig_2206_gwyn_tw, scientific = FALSE)`.

## Problem 3

#### Step One. Clean zip codes data

```{r}
# create a dataframe named "zip_code_df" for Zip Codes.csv data
# use 'clean_names()' to clean variable names
# use 'mutate()' to make sure FIPS and zip code variables are integer variables
# use 'select()' to reorder the dataframe, so that the date and state information are leading, then county info, then neighborhood info
# use 'rename()' to rename file_date and county for clarity
# use 'mutate()' to rename the date format for consistency with the zori_df
zip_code_df = 
  read_csv("data/zillow_data/Zip Codes.csv") |> 
  janitor::clean_names() |> 
  mutate(
    state_fips = as.integer(state_fips),
    county_fips = as.integer(county_fips),
    zip_code = as.integer(zip_code)
  ) |> 
  select(file_date, state_fips, county, county_code, county_fips, zip_code, neighborhood) |> 
  rename(zip_code_date = file_date, county_name_zipcode = county) |> 
  mutate(zip_code_date = "2007_07_25")

# check for uniqueness of zip code
duplicated_zip_code = zip_code_df |> 
  group_by(zip_code) |> 
  filter(n() > 1)
```

By reviewing the duplicated_zip_code table, we identify two repeated zip_code data (10463 and 11201). After checking, 10463 should be in Bronx, not New York County; 11201 should be in Kings, not New York County. Therefore, we remove the corresponded rows with the incorrect data.

```{r}
# use 'filter()' to remove the two incorrect rows
zip_code_df = zip_code_df |> 
  filter(!(zip_code == "11201" & county_name_zipcode == "New York"),
         !(zip_code == "10463" & county_name_zipcode == "New York"))
```

#### Step Two. Clean ZORI data

```{r}
# create a dataframe named "zori_df" for Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv data
# use 'clean_names()' to clean variable names
# use 'mutate()' to make sure some variables are integer variables
# use 'rename()' to rename region_name to be consistent with zip_code_df, and county_name, city for clarity
# use 'select()' to remove region_type (included when we rename region_name to zip_code) and state (repeated variable as state_name)
# use 'mutate()' to remove " County" in county_name to be consistent with zip_code_df
# use 'rename_with()' to rename the date variables for clarity
zori_df = 
  read_csv("data/zillow_data/Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv") |> 
  janitor::clean_names() |> 
  mutate(
    region_id = as.integer(region_id),
    size_rank = as.integer(size_rank),
    region_name = as.integer(region_name)
  ) |>
  rename(zip_code = region_name, county_name_zori = county_name, city_name = city) |> 
  select(-region_type, -state) |> 
  mutate(county_name_zori = str_remove(county_name_zori, " County$")) |> 
  rename_with(~ sub("^x", "", .x))

# check for uniqueness of zip code
duplicated_zori = zori_df |> 
  group_by(zip_code) |> 
  filter(n() > 1)
# there's no duplicated zip code!

# use pivot_longer() to convert wide to long data format for tidiness and readability 
# use 'separate()' to break "date" variable into "year", "month", "day" variables, separated by "_"
# use 'mutate()' to convert date into integer variables
# use 'mutate()' to replace month number with month name in "month"
# use 'select()' to remove the "day" variable, because it only represents the last day of the month
# use 'select()' to reorder the dataframe
zori_df = zori_df |> 
  pivot_longer(
    cols = "2015_01_31":"2024_08_31",
    names_to = "date",
    values_to = "zori"
  ) |> 
  separate(date, into = c("year", "month", "day"), sep = "_") |> 
  mutate(
    year = as.integer(year),
    month = as.integer(month),
    day = as.integer(day)
  ) |> 
  mutate(
    month = case_match(
    month,
    1 ~ "January",
    2 ~ "February",
    3 ~ "March",
    4 ~ "April",
    5 ~ "May",
    6 ~ "June",
    7 ~ "July",
    8 ~ "August",
    9 ~ "September",
    10 ~ "October",
    11 ~ "November",
    12 ~ "December"
    )
  ) |> 
  select(-day) |> 
  select(year, month, metro, state_name, city_name, county_name_zori, zip_code, region_id, size_rank, zori)
```

#### Step Three. Combine and interpret

```{r}
# create a dataframe named "zori_w_zip_df" for the combined data
# use 'left_join()' to merge zip code data into the zori data
# use 'select()' to remove some variables from zip_code_df: zip_code_date (not helpful in the combined dataset), state_fips, county_code (included in county_fips)
# use 'select()' to reorder the dataframe, so that it has the order of: date -> metro area -> state info -> city info -> county info -> neighborhood -> zip code & region info -> zori 
zori_w_zip_df = 
  left_join(zori_df, zip_code_df, by = "zip_code") |> 
  select(-zip_code_date, -state_fips, -county_code) |> 
  select(year, month, metro, state_name, city_name, county_name_zori, county_name_zipcode, county_fips, neighborhood, zip_code, everything())

# check for inconsistency between county_name_zipcode and county_name_zori
inconsistent_county = zori_w_zip_df |> 
  filter(county_name_zipcode != county_name_zori)

# use 'select()' to remove county_name_zipcode
# rename county_name_zori to county_name
zori_w_zip_df = zori_w_zip_df |> 
  select(-county_name_zipcode) |> 
  rename(county_name = county_name_zori)
```

We have found that for zip code 11693, county name on zori and zip code datasets are different. After checking, 11693 should be in Queens county, not Kings county, so we keep the county name in county_name_zori, and remove the column county_name_zipcode to avoid incorrect information and confusion.

```{r}
# check for ZIP codes in zip_code_df but not in zori_df
zori_missing_zip = zip_code_df |> 
  anti_join(zori_df, by = "zip_code")

# only keep zip_code, zori, county, and neighborhood data, in Jan 2020 and Jan 2021
# convert to wide format for comparison
# rename year variables and remove month column
# compare rental prices in January 2021 to prices in January 2020
covid_drop = zori_w_zip_df |> 
  filter(year %in% c(2020, 2021), month == "January") |> 
  select(year, month, zip_code, zori, county_name, neighborhood) |> 
  pivot_wider(
    names_from = year,
    values_from = zori
  ) |> 
  rename("2020_jan_zori" = "2020", "2021_jan_zori" = "2021") |> 
  select(-month) |> 
  mutate(zori_drop = `2021_jan_zori` - `2020_jan_zori`)

# average drop from Jan 2020 to 2021
mean_covid_drop = mean(covid_drop[["zori_drop"]], na.rm = TRUE)

# 10 ZIP codes with largest drop
# order the covid_drop df
# name county name to borough name
largest10_drop <- covid_drop |> 
  arrange(zori_drop) |> 
  slice_head(n = 10) |>
  select(zip_code, county_name, neighborhood, zori_drop) |> 
  mutate(
    county_name = case_match(
    county_name,
    "New York" ~ "Manhattan",
    "Kings" ~ "Brooklyn",
    "Richmond" ~ "Staten Island"
    )
  ) |> 
  rename(borough = county_name)
# show in a table
largest10_drop|> 
  knitr::kable()
```


**Interpretation:**

- The resulting tidy dataset contains 17284 rows and 12 columns. Within this dataset, `r sum(!is.na(zori_w_zip_df[["zori"]]))` rows have available ZORI data. Columns include date of observation (year, month), area information (metro, state_name, city_name), county information (county_name, county_fips), region information (neighborhood, zip_code, region_id, size_rank), and ZORI data (zori).

- There are `r length(unique(zori_w_zip_df[["zip_code"]]))` unique ZIP codes and `r length(unique(zori_w_zip_df[["neighborhood"]]))` unique neighborhoods included in the combined dataset.

- There are 171 ZIP codes that appear in the ZIP code dataset but not in the Zillow Rental Price dataset. For example, 10464 in the Bronx is located in Pelham Bay Park, which is not a residential area and therefore lacks rental price data. Similarly, 10111 in Manhattan corresponds to Rockefeller Center and nearby shopping areas, which are primarily commercial rather than residential. Another example is 11362 in Queens, covering the Little Neck neighborhood near Nassau County, where the rental market is limited and most housing is owner-occupied. In general, the Zillow dataset likely excludes ZIP codes that are not primarily residential, have low population density, or have very few rental listings and low needs.

- The average rental price in New York City dropped from January 2020 to 2021 is `r abs(mean_covid_drop)`. The 10 ZIP codes with the largest drop in rental prices from January 2020 to January 2021 are shown above. All of them are located in Manhattan, with the largest decline of 912.5966 occurring in a ZIP code where the average rent fell from 6334.211 in January 2020 to 5421.614 in January 2021. These ZIP codes are concentrated in mid- to lower Manhattan, covering neighborhoods such as the Lower East Side, Gramercy, Chelsea, Greenwich Village, and Soho.








